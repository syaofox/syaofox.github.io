---
title: "Linux Mint 22.2 CUDA + cuDNN 安装与版本管理教程"
created_at: "2025-10-28 02:22:40"
updated_at: "2025-10-28 03:23:06"
issue_number: 52
labels: ['tips']
url: https://github.com/syaofox/syaofox.github.io/issues/52
---

# Linux Mint 22.2 CUDA + cuDNN 安装与版本管理教程

# **Linux Mint 22.2 官方版 CUDA + cuDNN 安装与版本管理教程（2025 年最新）**

> **系统**：Linux Mint 22.2（基于 Ubuntu 24.04）  
> **核心原则**：
> - **系统级版本切换**：**官方 `update-alternatives` 机制**（APT 安装自动启用）
> - **项目级隔离**：**纯 `run.sh` 手动 `export` 路径**（轻量、无依赖）
> - **零警告、零冲突、官方推荐**

---

## 一、准备工作

```bash
# 更新系统
sudo apt update && sudo apt upgrade -y

# 安装基础工具
sudo apt install -y build-essential gcc g++ make wget curl python3-pip
```

检查 GPU：
```bash
lspci | grep -i nvidia
```

---

## 二、安装 NVIDIA 驱动（`nvidia-driver-580`）

### 推荐：图形化
1. 打开 **“驱动程序管理器”**
2. 选择 `nvidia-driver-580`（或更高）
3. 应用 → 重启

### 命令行
```bash
sudo apt install -y nvidia-driver-580
sudo reboot
```

验证：
```bash
nvidia-smi
# 显示驱动 580.xx，CUDA 上限 13.0+
```

---

## 三、安装 CUDA Toolkit（多版本并存）

```bash
# 添加官方仓库（Mint 22.2 = ubuntu2404）
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
```

安装版本（**推荐至少安装两个**）：
```bash
# 主版本
sudo apt install -y cuda-toolkit-13-0

# 兼容旧项目
sudo apt install -y cuda-toolkit-12-6
```

> 安装路径：
> - `/usr/local/cuda-13.0`
> - `/usr/local/cuda-12.6`

---

## 四、系统级版本切换（**官方 `update-alternatives`**）

> **APT 安装的 `cuda-toolkit-*` 自动启用此机制**

### 1. 查看当前指向

```bash
ls -la /usr/local/cuda
# 输出：/usr/local/cuda -> /etc/alternatives/cuda

ls -la /etc/alternatives/cuda
# 输出：/etc/alternatives/cuda -> /usr/local/cuda-13.0
```

### 2. 查看已安装版本

```bash
ls /usr/local | grep "^cuda-"
# 输出：
# cuda-12.6
# cuda-13.0
```

### 3. 将版本加入 `update-alternatives`（只需一次）

```bash
sudo update-alternatives --install /usr/local/cuda cuda /usr/local/cuda-13.0 130
sudo update-alternatives --install /usr/local/cuda cuda /usr/local/cuda-12.6 126
```

> 数字 `130`、`126` 是优先级，越大越优先。

### 4. **官方切换命令**

```bash
sudo update-alternatives --config cuda
```

输出示例：
```
  选择       路径                   优先级  状态
------------------------------------------------------------
* 0           /usr/local/cuda-13.0   130     自动模式
  1           /usr/local/cuda-12.6   126     手动模式
  2           /usr/local/cuda-13.0   130     手动模式

要维持当前值[*]请按<回车>，或者键入选择的编号：1
```

输入 `1` → 切换到 CUDA 12.6

### 5. 验证切换

```bash
nvcc --version
# 显示 release 12.6
```

---

## 五、安装 cuDNN（自动支持当前 CUDA）

```bash
sudo apt install -y cudnn9-cuda-13
sudo apt install -y cudnn9-cuda-12
```

> 库自动安装至 `/usr/lib/x86_64-linux-gnu/`，无需手动配置。

---

## 六、项目级隔离：**纯 `run.sh` 手动 `export`**

> **推荐方式**：**不依赖系统默认，每个项目独立指定 CUDA 路径**

### 目录结构

```bash
~/projects/
├── ml_model_a/         # 用 CUDA 13.0
│   └── run.sh
├── legacy_app/         # 用 CUDA 12.6
│   └── run.sh
└── cpu_task/           # 无 GPU
    └── run.sh
```

---

### 1. `ml_model_a/run.sh`（强制用 CUDA 13.0）

```bash
#!/bin/bash
# 项目：ml_model_a → 强制使用 CUDA 13.0

# 清理旧 CUDA 环境
unset CUDA_HOME
export PATH=$(echo $PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')
export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')

# 手动指定 CUDA 13.0
export CUDA_HOME=/usr/local/cuda-13.0
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

echo "项目 ml_model_a → CUDA 13.0"
nvcc --version | head -n 2

# 你的代码
python3 train.py
```

---

### 2. `legacy_app/run.sh`（强制用 CUDA 12.6）

```bash
#!/bin/bash
# 项目：legacy_app → 强制使用 CUDA 12.6

unset CUDA_HOME
export PATH=$(echo $PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')
export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')

export CUDA_HOME=/usr/local/cuda-12.6
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

echo "项目 legacy_app → CUDA 12.6"
nvcc --version | head -n 2

python3 old_script.py
```

---

### 3. `cpu_task/run.sh`（禁用 GPU）

```bash
#!/bin/bash
# 项目：cpu_task → 禁用 GPU

unset CUDA_HOME CUDA_VISIBLE_DEVICES
export PATH=$(echo $PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')
export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -v "/cuda" | tr '\n' ':' | sed 's/:$//')

echo "GPU 已禁用"
python3 cpu_only.py
```

---

### 使用方法

```bash
cd ~/projects/ml_model_a
chmod +x run.sh
./run.sh
```

> **完全隔离**：系统默认 13.0，项目用 12.6，互不影响。

---

## 七、验证安装

```bash
sudo reboot
```

### 验证 CUDA
```bash
nvcc --version
sudo apt install -y cuda-samples-13-0
cd /usr/local/cuda/samples/1_Utilities/deviceQuery
make && ./deviceQuery  # Result = PASS
```

### 验证 cuDNN
```bash
sudo apt install -y libcudnn9-samples
cp -r /usr/share/doc/libcudnn9-samples/mnistCUDNN ~/
cd ~/mnistCUDNN
make && ./mnistCUDNN  # Test passed!
```

---

## 八、框架安装（在 `run.sh` 中）

```bash
# PyTorch CUDA 13.0
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# PyTorch CUDA 12.6
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# TensorFlow
pip install tensorflow[and-cuda]
```

---

## 九、卸载

```bash
# 卸载 CUDA
sudo apt --purge remove "*cuda*" "*cudnn*" -y

# 卸载驱动
sudo apt --purge remove "nvidia-driver-*" -y
sudo apt autoremove -y

# 清理 alternatives
sudo update-alternatives --remove-all cuda

# 删除项目
rm -rf ~/projects/*/run.sh
```

---

## 十、总结：官方 + 手动隔离

| 目标 | 官方命令 |
|------|----------|
| **系统切换版本** | `sudo update-alternatives --config cuda` |
| **查看当前版本** | `ls -la /etc/alternatives/cuda` |
| **添加新版本** | `sudo update-alternatives --install /usr/local/cuda cuda /usr/local/cuda-XX.X XXX` |
| **项目隔离** | `run.sh` 中 `export CUDA_HOME=/usr/local/cuda-XX.X` |

---

## 最终目录结构

```
/usr/local/
├── cuda -> /etc/alternatives/cuda      # 官方管理
└── cuda-13.0/                          # 真实目录
└── cuda-12.6/
~/projects/
├── ml_model_a/run.sh                   # export cuda-13.0
├── legacy_app/run.sh                   # export cuda-12.6
└── cpu_task/run.sh                     # unset CUDA
```

---

**完结！**  
> **系统用官方 `update-alternatives` 切换**  
> **项目用 `run.sh` 手动 `export` 隔离**  
> **最规范、最稳定、最灵活**


